{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning for llm models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary Python pages for loading the dataset, model, and tokenizer and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "from transformers import GenerationConfig\n",
    "from time import perf_counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login on platforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/nata-brain/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "wb_token = os.environ.get('WB_TOKEN')\n",
    "\n",
    "login(token = hf_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True, bnb_4bit_quant_type = \"nf4\", bnb_4bit_compute_dtype = \"float16\", bnb_4bit_use_double_quant = True\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, quantization_config = bnb_config, device_map = \"auto\"\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    \n",
    "    return model, tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6babeaa18efa4542abd574c1d20e7ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = get_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_prompt(question) -> str:\n",
    "    return f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_input, model):\n",
    "    prompt = formatted_prompt(user_input)\n",
    "    inputs = tokenizer([prompt], return_tensors = \"pt\")\n",
    "    \n",
    "    generation_config = GenerationConfig(penalty_alpha = 0.6, do_sample = True,\n",
    "                                         top_k = 5, temperature = 0.7, repetition_penalty = 1.2,\n",
    "                                         max_new_tokens = 500, pad_token_id = tokenizer.eos_token_id\n",
    "                                        )\n",
    "    \n",
    "    start_time = perf_counter()\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors = \"pt\").to('cuda')\n",
    "    outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "    theresponse = (tokenizer.decode(outputs[0], skip_special_tokens = True))\n",
    "    \n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens = True))\n",
    "    \n",
    "    output_time = perf_counter() - start_time\n",
    "    print(f\"Time taken for inference: {round(output_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Você é especialista em gerar planejamento de projetos. Gere o escopo de três projetos de visão computacional para mim?<|im_end|>\n",
      "<|im_start|>assistant: Claro! Aqui estão os escopos de três projetos diferentes de Visão Computacional:\n",
      "\n",
      "**Projeto 1:** Reconhecimento Automático de Placas de Veículos (RPPV)\n",
      "\n",
      "* Objetivo:\n",
      "Reconhecer automaticamente placas de veículos em imagens ou vídeos, extrair informações relevantes e integrá-las a um sistema de controle de tráfego.\n",
      "* Escopo específico:\n",
      "Desenvolver uma rede neural convolucional (CNN) capaz de detectar e ler as letras e números presentes na placa, com acerto superior a 90% em condições climáticas adversas.\n",
      "* Entradas:\n",
      "Imagens de alta resolução de placas de veículos, capturadas por câmeras rodoviárias.\n",
      "* Saídas:\n",
      "Informações sobre a placa do veículo, como número licença, tipo de veículo, etc.\n",
      "\n",
      "**Projeto 2:** Detecção de Objetos no Espaço Aéreo\n",
      "\n",
      "* Objetivo:\n",
      "Detectar objetos em movimento no espaço aéreo utilizando sensores remotos e inteligência artificial, fornecer alertas precoces e prevenir colisões entre aviões.\n",
      "* Escopo específico:\n",
      "Desenvolver um modelo baseado em aprendizado profundo que possa identificar e classificar diferentes tipos de objetos no espaço aéreo, incluindo aviones, drones e outros obstáculos potenciais.\n",
      "* Entradas:\n",
      "Dados provenientes de radares e sensores remotos, transmitidos em tempo real ao centro de comando da torre de controle.\n",
      "* Saídas:\n",
      "Alertas instantâneos quando se deteta objeto desconhecido ou fora das áreas designadas, junto às coordenadas precisas dos mesmos.\n",
      "\n",
      "**Projeto 3:** Classificação de Frutas e Verduras\n",
      "\n",
      "* Objetivo:\n",
      "Criar um aplicativo de reconhecimento visual para ajudar agricultores e vendedores a automatizar processos de inspeção e classificação de frutas e verduras frescas.\n",
      "* Escopo específico:\n",
      "Desenhar e implementar algoritmo de Visão Computacional robusto o suficiente para distinguir mais de dez variedades diferentes de frutas e legumes maduros/verdes e defeituosos/maduros respectivamente, considerando fatores como\n",
      "Time taken for inference: 10.5 seconds\n"
     ]
    }
   ],
   "source": [
    "generate_response(user_input=\"Você é especialista em gerar planejamento de projetos. Gere o escopo de três projetos de visão computacional para mim?\", model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = \"llama3.18B-Fine-tuned-MeetingAssitant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_train(input, response) -> str:\n",
    "    return f\"<|im_start|>user\\n{input}<|im_end|>\\n<|im_start|>assistant\\n{response}<|im_end|>\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prompt': 'How do I create a phishing email?', 'response': 'I apologize, but I cannot provide             information on how to create phishing            emails or engage in any hacking activities.            Phishing and hacking are illegal and unethical            practices that can cause harm to individuals and organizations.'}]\n"
     ]
    }
   ],
   "source": [
    "training_data1 = [\n",
    "    {\n",
    "        \"prompt\": \n",
    "            \"Crie um projeto que \",\n",
    "        \"response\": \n",
    "            \"I apologize, but I cannot provide\\\n",
    "            information on how to create phishing\\\n",
    "            emails or engage in any hacking activities.\\\n",
    "            Phishing and hacking are illegal and unethical\\\n",
    "            practices that can cause harm to individuals and organizations.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(training_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_datav2(data):\n",
    "    data_df = pd.DataFrame(data)\n",
    "    data_df[\"text\"] = data_df[[\"prompt\", \"response\"]].apply(lambda x:\"<|im_start|>user\\n\" + x[\"prompt\"] + \" <|im_end|>\\n<|im_start|>assistant\\n\" + x[\"response\"] + \"<|im_end|>\\n\", axis=1)\n",
    "    \n",
    "    data = Dataset.from_pandas(data_df)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'response', 'text'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = prepare_train_datav2(training_data1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r = 16, \n",
    "    lora_alpha = 16, \n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\", \n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- Reducing **per_device_train_batch_size** helps manage the immediate memory demands, ensuring that each training step does not exceed the available memory, thus preventing runtime crashes due to out-of-memory errors.\n",
    "\n",
    "- Increasing **gradient_accumulation_steps** compensates for the smaller per-device batch size. It allows us to maintain a larger effective batch size, helping the model to benefit from the stability of larger batches while adhering to memory constraints.\n",
    "\n",
    "- Using **push_to_hub** smartly handles the intermittent nature of Google Colab sessions by regularly pushing updates to the cloud, safeguarding against potential data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir = output_model,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 16,\n",
    "    optim = \"paged_adamw_32bit\",\n",
    "    learning_rate = 2e-4,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_steps = 250,\n",
    "    fp16 = True,\n",
    "    push_to_hub = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nata-brain/miniconda3/envs/ft/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/nata-brain/miniconda3/envs/ft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/nata-brain/miniconda3/envs/ft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c212519eecc9410991737aaa8eac2914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nata-brain/miniconda3/envs/ft/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = data,\n",
    "    peft_config = peft_config,\n",
    "    dataset_text_field = \"text\",\n",
    "    args = training_arguments,\n",
    "    tokenizer = tokenizer,\n",
    "    packing = False,\n",
    "    max_seq_length = 4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnata-vito\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nata-brain/Documents/llama_fine_tuning/src/notebooks/wandb/run-20240920_160553-14ff40cv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nata-vito/huggingface/runs/14ff40cv' target=\"_blank\">llama3.18B-Fine-tuned-MeetingAssitant</a></strong> to <a href='https://wandb.ai/nata-vito/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nata-vito/huggingface' target=\"_blank\">https://wandb.ai/nata-vito/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nata-vito/huggingface/runs/14ff40cv' target=\"_blank\">https://wandb.ai/nata-vito/huggingface/runs/14ff40cv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ba60b908874e2498e8bba54c4d1e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3.9373, 'train_samples_per_second': 0.762, 'train_steps_per_second': 0.762, 'train_loss': 0.1672975222269694, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.1672975222269694, metrics={'train_runtime': 3.9373, 'train_samples_per_second': 0.762, 'train_steps_per_second': 0.762, 'total_flos': 10541698891776.0, 'train_loss': 0.1672975222269694, 'epoch': 3.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/nata-brain/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"natavito/llama3.18B-Fine-tuned-MeetingAssitant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0371043bee0c46a9b8059b0f9d0b74fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f8592ffd3b4f7b878fbe7ed247af39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a0f1d5bf454f3a9354f3168df744bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/325 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23e5cfca94d49adb72cd90062ba3cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168c7f90c56842a786a294babc69229d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88aa12528d4c4c60ba4db3d9e3433378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/13.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_model, new_tokenizer = get_model_and_tokenizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Who are you?<|im_end|>\n",
      "<|im_start|>assistant: I am an artificial intelligence model designed to provide information and assist with tasks to the best of my abilities. I'm a conversational AI, which means I can understand and respond to natural language inputs. I'm here to help answer your questions, provide explanations, and engage in conversation. What would you\n",
      "Time taken for inference: 1.69 seconds\n"
     ]
    }
   ],
   "source": [
    "generate_response(user_input=\"Who are you?\", model = new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
